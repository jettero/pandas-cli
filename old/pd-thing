#!/usr/bin/env python
# encoding: utf-8

import sys
import argparse
import subprocess
import pandas as pd
from tabulate import tabulate
from collections import namedtuple

HEADERS = list()
args = False


def qprint(*a, **kw):
    if not args.quiet:
        kw["file"] = sys.stderr
        print(*a, **kw)


class Dat(namedtuple("Dat", ["fname", "df"])):
    def __repr__(self):
        return f"Dat<{self.fname}>"


def my_read_csv(fname):
    global HEADERS
    df = pd.read_csv(fname)
    if not HEADERS:
        # we should somewhere mention in --help that HEADERS only populates
        # form the first file with headers
        HEADERS = df.columns.tolist()
    return Dat(fname, df)


def my_nh_read_csv(fname):
    df = pd.read_csv(fname, header=None, names=HEADERS)
    return Dat(fname, df)


def populate_args(*a, **kw):
    parser = argparse.ArgumentParser(formatter_class=argparse.ArgumentDefaultsHelpFormatter)

    parser.add_argument(
        "--csv",
        "-c",
        action="extend",
        dest="files",
        metavar="FILE",
        nargs="+",
        type=my_read_csv,
        help="csv input files",
    )
    parser.add_argument(
        "--csv-no-header",
        "--cnh",
        "-n",
        dest="files",
        metavar="FILE",
        action="extend",
        nargs="+",
        type=my_nh_read_csv,
        help="csv input files that lack headers -- try to use headers from previous files",
    )
    parser.add_argument(
        "-o",
        "--output",
        default=":orgtbl",
        help="output filename :- if the filename starts with a "
        "':', the argument will be considered a tabulate format name instead. "
        "Addtionally, '-' or ':csv' will convince this program to print csv to stdout. "
        "Similarly, ':tsv' will convince the program to print a <tab> table.",
    )
    parser.add_argument("--debug-args", action="store_true", help="show the args object without doing anything else")
    parser.add_argument(
        "-u",
        "--unique",
        type=str,
        nargs="*",
        help="names of fields upon which we should uniqify things. "
        "e.g., -u CustomerID to make sure CustomerIDs only occur once in the output",
    )

    group = parser.add_mutually_exclusive_group()
    group.add_argument(
        "-l",
        "--list",
        action="store_const",
        dest="mode",
        const="list",
        help="useful for figuring out which file is which position",
    )
    group.add_argument(
        "--filtered",
        action="store_const",
        dest="mode",
        const="A-cat",
        help="A-(B+C+D+...) mode. We're interested in all the items in the first file that aren't in any of the following",
    )
    group.add_argument(
        "--column-merge",
        action="store_const",
        dest="mode",
        const="column-merge",
        help="Concatenate the various input files columnwise. The specified key columns will inform the process of appending missing columns (or empty fields in those columns).",
    )
    group.add_argument(
        "--cat",
        "--concatenate",
        action="store_const",
        dest="mode",
        const="cat",
        help="A+B+C=D mode. All the items concatenate together",
    )

    parser.add_argument(
        "--merge-type",
        choices=["left", "right", "outer", "inner", "cross"],
        # * left: use only keys from left frame, similar to a SQL left outer join;
        #         preserve key order.
        # * right: use only keys from right frame, similar to a SQL right outer join;
        #          preserve key order.
        # * outer: use union of keys from both frames, similar to a SQL full outer
        #          join; sort keys lexicographically.
        # * inner: use intersection of keys from both frames, similar to a SQL inner
        #          join; preserve the order of the left keys.
        # * cross: creates the cartesian product from both frames, preserves the order
        #          of the left keys.
        type=str,
        default="outer",
        help="The type of cross product (in the relational calculus sense) we want to use",
    )

    parser.add_argument(
        "-s",
        "--show-each-file",
        action="store_true",
        default=False,
        help="show each input file as a table",
    )

    parser.add_argument(
        "--max-output-table-lines",
        "--max-lines",
        "-m",
        type=int,
        default=15,
        help="max lines to show when showing a dataframes as tables",
    )

    parser.add_argument(
        "--super-pandas",
        action="store_true",
        default=False,
        help="attempt to force install/update pandas[excel,plot,output-formatting,computation,html,xml,"
        "hdf5,aws,gcp,clipboard,compression,performance,computation] using whatever pip happens to be in your path",
    )

    parser.add_argument(
        "-q",
        "--quiet",
        action="store_true",
        default=False,
        help="suppress printing most actions to stderr as they happen",
    )

    global args
    args = parser.parse_args(*a, **kw)

    if not args.mode:
        args.mode = "cat"

    if args.show_each_file:
        for fname, df in args.files:
            output(df, force_table=True, describe_as=fname)

    if args.debug_args:
        # NOTE: cannot qprint() anywhere until the global args variable is set
        print(args, file=sys.stderr)
        sys.exit(0)

    if args.super_pandas:
        p = subprocess.Popen(
            [
                "pip",
                "install",
                "-U",
                "psycopg2-binary",
                "pandas[excel,plot,output-formatting,computation,html,xml,hdf5,aws,gcp,clipboard,compression,performance]",
                "--force-reinstall",
            ]
        )
        p.communicate()
        sys.exit(0)


def pluralize(word, listical):
    count = listical if isinstance(listical, (int,)) else len(listical)
    if count > 1:
        return word + "s"
    return word


def output(df, describe_as=None, force_table=False):
    if args.unique:
        qprint(f"uniqifying on key {pluralize('field', args.unique)}: {', '.join(args.unique)}")
        df = df.drop_duplicates(subset=args.unique)  # NOTE: we also do this drop on the initial input files
    if args.output in ("-", ":csv"):
        return df.to_csv(sys.stdout, index=False)
    if args.output in (":tsv"):
        return df.to_csv(sys.stdout, sep="\t", index=False)
    if args.output in (":xls", ":xlsx"):
        return df.to_excel(sys.stdout)
    if args.output.startswith(":") or force_table:
        fmt = args.output[1:] if args.output.startswith(":") else "orgtbl"
        if len(df) > args.max_output_table_lines:  # we could quibble about the header and separator lines I suppose
            ellipis_row = pd.DataFrame([{col: "â‹®" for col in df.columns}])
            df = pd.concat((df.head(args.max_output_table_lines), ellipis_row), ignore_index=True)
        if describe_as:
            qprint(f"\n-----=: {describe_as}")
            qprint(tabulate(df, df.columns.tolist(), tablefmt=fmt, showindex=False))
            qprint("")
        else:
            qprint("")
            print(tabulate(df, df.columns.tolist(), tablefmt=fmt, showindex=False))
        return
    df.to_csv(args.output, index=False)
    l = len(df)
    qprint(f'wrote {l} {pluralize("line", l)} output to {args.output}')


def main():
    if args.mode == "list":
        fmt = args.output[1:] if args.output.startswith(":") else "orgtbl"
        return print(
            tabulate(
                [
                    [i, dat.fname, len(dat.df), ", ".join(dat.df.columns.tolist()[1:])]
                    for i, dat in enumerate(args.files)
                ],
                ["fno", "fname", "lines", "fields"],
                tablefmt=fmt,
            )
        )

    if args.mode == "cat":
        qprint(f"concatenating input files: {', '.join(x.fname for x in args.files)}")
        df = pd.concat((x.df for x in args.files), ignore_index=True)
        return output(df)

    if args.mode == "A-cat":
        B = pd.concat((x.df for x in args.files[1:]), ignore_index=True)
        A = args.files[0].df
        k = args.unique if args.unique else HEADERS
        if len(k) == 1:
            k = k[0]
            qprint(
                f'filtering items from {args.files[0].fname} based on "{k}" values '
                f"from {', '.join(x.fname for x in args.files[1:])}"
            )
            return output(A[~A[k].isin(B[k])])
        if len(k) > 1:
            A["unique_key"] = A[k].apply(tuple, axis=1)
            B["unique_key"] = B[k].apply(tuple, axis=1)
            qprint(
                f'filtering items from {args.files[0].fname} based on "{k}" values '
                f"from {', '.join(x.fname for x in args.files[1:])}"
            )
            return output(A[~A.unique_key.isin(B.unique_key)].drop(columns="unique_key"))

    if args.mode == "column-merge":
        A = args.files[0].df
        k = args.unique
        if not args.unique:
            qprint(f"key columns unspecified, guessing")
            k = [x for x in HEADERS if x.endswith("id")]
            if not k:
                k = HEADERS
        qprint(f"merging all columns (keyed on {'-'.join(sorted(k))}) starting with {args.files[0].fname}")
        DING_DING_DING = "\x07\x07\x07"  # ding ding ding
        for fname, B in args.files[1:]:
            qprint(f"  merge in the columns of {fname}")
            A = pd.merge(A, B, on=k, suffixes=("", DING_DING_DING), how=args.merge_type)
            c = A.columns.tolist()
            d = [x for x in c if f"{x}{DING_DING_DING}" in c]
            for lhs in d:
                A[lhs] = A[lhs].combine_first(A[f"{lhs}{DING_DING_DING}"])
            A = A.drop(columns=[f"{x}{DING_DING_DING}" for x in d])
        return output(A)

    print(
        f'ERROR: mode="{args.mode}" not understood. pick one of --list or --cat or whatever... '
        "this is a bug either way though",
        file=sys.stderr,
    )


if __name__ == "__main__":
    try:
        populate_args()
        main()
    except KeyboardInterrupt:
        pass
